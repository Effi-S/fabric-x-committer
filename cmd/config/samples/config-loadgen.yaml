orderer-client:
  sidecar-endpoint: localhost:5000
  coordinator:
    endpoint: localhost:5001
  orderer:
    endpoints:
      - localhost:1234
      - localhost:1235
    consensus-type: BFT
    channel-id: channel
  broadcast-parallelism: 50
monitoring:
  server:
    endpoint: localhost:5002
  latency:
    sampler:
      type: timer
      sampling-interval: 10s
    buckets:
      type: uniform
      max-latency: 5s
      bucket-count: 1000
load-profile:
  key:
    size: 32
  block:
    size: 500
  transaction:
    read-write-count:
      type: constant
      const: 2
    policy:
      namespace-policies:
        0:
          scheme: ECDSA
          seed: 10
        1024:
          scheme: ECDSA
          seed: 11
  conflicts:
    invalid-signatures: 0.1
  seed: 12345
  # We use small number of workers to reduce the CPU load during tests.
  workers: 1
stream:
  rate-limit:
    initial-limit: 1000
  # We set low values for the buffer and batch to reduce the CPU load during tests.
  buffers-size: 1
  gen-batch: 1
generate:
  namespaces: true
  load: true

# The stopping condition for generating load according to the collected metrics.
# Zero value indicate no limit.
# The limit on the number of TXs is applied at block granularity. I.e., more TXs might be created than expected
# if a block overshot.
# The load generator stops when both requirements are met, i.e., one of them might overshoot.
# For the orderer adapter, the blocks limit is ignored for broadcasting as we don't track submitted blocks.
# For adapters that use concurrent submitters, we cannot enforce exact limits.
# The sidecar and coordinator adapters are sequential, so they don't have these issues.
limit:
  blocks: 10_000
  transactions: 0
